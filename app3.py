import streamlit as st
import torch
import os
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_ollama import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
from langchain_core.documents import Document

# ðŸ”¹ Force GPU usage for Ollama (if available)
os.environ["OLLAMA_CUDA"] = "1" if torch.cuda.is_available() else "0"

# ðŸ”¹ Ensure CUDA availability for Torch
device = "cuda" if torch.cuda.is_available() else "cpu"
st.write(f"Using device: {device}")

# ðŸ”¹ Chat assistant prompt template
prompt_template = """
You are an AI assistant for answering questions based on provided PDF content. 
Use the following retrieved context to answer concisely in three sentences maximum.
If the answer isn't in the context, say "I don't know."

Question: {question} 
Context: {context} 
Answer:
"""

# ðŸ”¹ Directory to store uploaded PDFs
pdf_storage_dir = "uploaded_pdfs/"
os.makedirs(pdf_storage_dir, exist_ok=True)

# ðŸ”¹ Initialize embeddings & model
embeddings_model = OllamaEmbeddings(model="deepseek-r1:7b")
vector_store = InMemoryVectorStore(embedding=embeddings_model)  # ðŸ”¹ Fixed initialization
qa_model = OllamaLLM(model="deepseek-r1:7b")

def save_uploaded_file(uploaded_file):
    """Save the uploaded PDF file to storage."""
    file_path = os.path.join(pdf_storage_dir, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

def load_pdf_content(pdf_path):
    """Load text from a PDF using PDFPlumber."""
    loader = PDFPlumberLoader(pdf_path)
    documents = loader.load()
    return [doc.page_content for doc in documents]  # ðŸ”¹ Extract text properly

def split_into_chunks(texts):
    """Convert raw text into LangChain Document objects and split into chunks."""
    documents = [Document(page_content=text) for text in texts]
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200, add_start_index=True
    )
    return text_splitter.split_documents(documents)

def index_pdf_documents(chunks):
    """Index documents into the vector store for retrieval."""
    vector_store.add_documents(chunks)

def retrieve_relevant_chunks(query):
    """Retrieve top similar chunks related to the user's question."""
    return vector_store.similarity_search(query, k=3)  # ðŸ”¹ Fetch top 3 relevant chunks

def generate_answer(question, relevant_chunks):
    """Generate an AI response based on retrieved PDF content."""
    context = "\n\n".join([doc.page_content for doc in relevant_chunks])
    prompt = ChatPromptTemplate.from_template(prompt_template)
    response_chain = prompt | qa_model
    response = response_chain.invoke({"question": question, "context": context})
    
    return response["text"] if isinstance(response, dict) else response  # ðŸ”¹ Ensure clean output

# ðŸ”¹ Streamlit UI
st.title("ðŸ“„ Chat with Your PDF")

uploaded_file = st.file_uploader("Upload a PDF file", type="pdf", accept_multiple_files=False)

if uploaded_file:
    # Save and process PDF
    pdf_path = save_uploaded_file(uploaded_file)
    docs = load_pdf_content(pdf_path)
    doc_chunks = split_into_chunks(docs)
    index_pdf_documents(doc_chunks)

    user_query = st.chat_input("Ask a question about the PDF...")

    if user_query:
        st.chat_message("user").write(user_query)
        matched_chunks = retrieve_relevant_chunks(user_query)
        
        if matched_chunks:
            response = generate_answer(user_query, matched_chunks)
        else:
            response = "I couldn't find relevant information in the document."

        st.chat_message("assistant").write(response)
